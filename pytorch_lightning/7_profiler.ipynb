{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPYW8jE3hz2wyZ6BRjMLvr1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alfie1104/deeplearning-with-pytorch/blob/main/pytorch_lightning/7_profiler.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install lightning"
      ],
      "metadata": {
        "id": "kA24-uwgjmgG"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install torchmetrics"
      ],
      "metadata": {
        "id": "tBBIzCLXqv0u"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install tensorboard"
      ],
      "metadata": {
        "id": "nE17l4ubyzl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install torch-tb-profiler"
      ],
      "metadata": {
        "id": "3ZzghUQq1g4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## config.py"
      ],
      "metadata": {
        "id": "1cxoCYuiyqpl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training hyperparameters\n",
        "INPUT_SIZE = 784\n",
        "NUM_CLASSES = 10\n",
        "LEARNING_RATE = 0.001\n",
        "BATCH_SIZE = 64\n",
        "NUM_EPOCHS = 1\n",
        "\n",
        "# Dataset\n",
        "DATA_DIR = \"dataset/\"\n",
        "NUM_WORKERS = 4\n",
        "\n",
        "# Compute related\n",
        "ACCELERATOR = \"gpu\"\n",
        "DEVICES = [0] # single gpu. it is the same as DEVICE = 1\n",
        "PRECISION = 16"
      ],
      "metadata": {
        "id": "TnBoDDCIyc0-"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## dataset.py"
      ],
      "metadata": {
        "id": "T7mgZCx50KkS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import random_split\n",
        "import lightning as pl\n",
        "\n",
        "class MnistDataModule(pl.LightningDataModule):\n",
        "  def __init__(self, data_dir, batch_size, num_workers):\n",
        "    super().__init__()\n",
        "    self.data_dir = data_dir\n",
        "    self.batch_size = batch_size\n",
        "    self.num_workers = num_workers\n",
        "\n",
        "  def prepare_data(self):\n",
        "    # single gpu\n",
        "    datasets.MNIST(self.data_dir, train=True, download=True)\n",
        "    datasets.MNIST(self.data_dir, train=False, download=True)\n",
        "\n",
        "  def setup(self, stage):\n",
        "    # multiple gpu\n",
        "    entire_dataset = datasets.MNIST(\n",
        "        root=self.data_dir,\n",
        "        train=True,\n",
        "        transform=transforms.Compose([\n",
        "            transforms.RandomVerticalFlip(),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "        ]),\n",
        "        download=False,\n",
        "    )\n",
        "    self.train_ds, self.val_ds = random_split(entire_dataset, [50000, 10000])\n",
        "    self.test_ds = datasets.MNIST(\n",
        "        root=self.data_dir,\n",
        "        train=False,\n",
        "        transform=transforms.ToTensor(),\n",
        "        download=False,\n",
        "    )\n",
        "\n",
        "  def train_dataloader(self):\n",
        "    return DataLoader(self.train_ds, batch_size=self.batch_size, num_workers=self.num_workers, shuffle=False)\n",
        "\n",
        "  def val_dataloader(self):\n",
        "    return DataLoader(self.val_ds, batch_size=self.batch_size, num_workers=self.num_workers, shuffle=False)\n",
        "\n",
        "  def test_dataloader(self):\n",
        "    return DataLoader(self.test_ds, batch_size=self.batch_size, num_workers=self.num_workers, shuffle=False)"
      ],
      "metadata": {
        "id": "_3mEmzqm0Oam"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## model.py"
      ],
      "metadata": {
        "id": "7bABgXnU0S6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn, optim\n",
        "import lightning as pl\n",
        "import torchmetrics\n",
        "import torchvision\n",
        "\n",
        "class NN(pl.LightningModule):\n",
        "  def __init__(self, input_size, learning_rate, num_classes):\n",
        "    super().__init__()\n",
        "    self.lr = learning_rate\n",
        "    self.fc1 = nn.Linear(input_size, 50)\n",
        "    self.fc2 = nn.Linear(50, num_classes)\n",
        "    self.loss_fn = nn.CrossEntropyLoss()\n",
        "    self.accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
        "    self.f1_score = torchmetrics.F1Score(task=\"multiclass\", num_classes=num_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = self.fc2(x)\n",
        "    return x\n",
        "\n",
        "  def training_step(self, batch, batch_idx):\n",
        "    x, y = batch\n",
        "    loss, scores, y = self._common_step(batch, batch_idx)\n",
        "\n",
        "    self.log_dict({'train_loss':loss},\n",
        "                  on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
        "\n",
        "    if batch_idx % 100 == 0:\n",
        "      x = x[:8] # 8 images\n",
        "      grid = torchvision.utils.make_grid(x.view(-1,1,28,28)) # 8 images into single image\n",
        "      self.logger.experiment.add_image(\"mnist_images\",grid, self.global_step)\n",
        "\n",
        "    return {'loss' : loss, 'scores':scores, 'y':y}\n",
        "\n",
        "  def training_epoch_end(self, outputs):\n",
        "    scores = torch.cat([x[\"scores\"] for x in outputs])\n",
        "    y = torch.cat([x[\"y\"] for x in outputs])\n",
        "    self.log_dict(\n",
        "        {\n",
        "            \"train_acc\": self.accuracy(scores, y),\n",
        "            \"train_f1\": self.f1_score(scores, y),\n",
        "        },\n",
        "        on_step=False,\n",
        "        on_epoch=True,\n",
        "        prog_bar=True,\n",
        "    )\n",
        "\n",
        "  def validation_step(self, batch, batch_idx):\n",
        "    loss, scores, y = self._common_step(batch, batch_idx)\n",
        "    self.log('val_loss', loss)\n",
        "    return loss\n",
        "\n",
        "  def test_step(self, batch, batch_idx):\n",
        "    loss, scores, y = self._common_step(batch, batch_idx)\n",
        "    self.log('test_loss', loss)\n",
        "    return loss\n",
        "\n",
        "  def _common_step(self, batch, batch_idx):\n",
        "    x, y = batch\n",
        "    x = x.reshape(x.size(0), -1) # flatten the matrix x to array\n",
        "    scores = self.forward(x)\n",
        "    loss = self.loss_fn(scores, y)\n",
        "    # in PyTorch lightning we don't need to care of 'zero_grad','backward','step'\n",
        "    return loss, scores, y\n",
        "\n",
        "  def predict_step(self, batch, batch_idx):\n",
        "    x, y = batch\n",
        "    x = x.reshape(x.size(0), -1) # flatten the matrix x to array\n",
        "    scores = self.forward(x)\n",
        "    preds = torch.argmax(scores, dim=1)\n",
        "    return preds\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "    return optim.Adam(self.parameters(), lr=self.lr)"
      ],
      "metadata": {
        "id": "8hNWQBSs0UtT"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## callbacks.py"
      ],
      "metadata": {
        "id": "CbnIFlJp9kpl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from lightning.pytorch.callbacks import EarlyStopping, Callback\n",
        "\n",
        "class MyPrintingCallback(Callback):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def on_train_start(self, trainer, pl_module):\n",
        "    print(\"Starting to train!\")\n",
        "\n",
        "  def on_train_end(self, trainer, pl_module):\n",
        "    print(\"Training is done.\")"
      ],
      "metadata": {
        "id": "pnjsBKf_9oJm"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## train.py"
      ],
      "metadata": {
        "id": "cCsqyP6u1StM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import lightning as pl\n",
        "from lightning.pytorch.loggers import TensorBoardLogger\n",
        "from lightning.pytorch.profilers import PyTorchProfiler\n",
        "\n",
        "torch.set_float32_matmul_precision(\"medium\") # to make lightning happy\n",
        "\n",
        "# 파일로 분리했을 때 아래 주석 해제 필요\n",
        "# from model import NN\n",
        "# from dataset import MnistDataModule\n",
        "# import config\n",
        "# from callbacks import MyPrintingCallback, EarlyStopping\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#   logger = TensorBoardLogger(\"tb_logs\",name=\"mnist_model_v0\")\n",
        "#   # Set device cuda for GPU if it's available otherwise run on the CPU\n",
        "#   device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "#   # Initialize network\n",
        "#   model = NN(input_size=config.INPUT_SIZE, learning_rate=config.LEARNING_RATE, num_classes=config.NUM_CLASSES)\n",
        "\n",
        "#   # Data Module\n",
        "#   dm = MnistDataModule(data_dir=config.DATA_DIR, batch_size=config.BATCH_SIZE, num_workers=config.NUM_WORKERS)\n",
        "\n",
        "#   trainer = pl.Trainer(profiler=profiler, logger=logger, accelerator=config.ACCELERATOR, devices=config.DEVICES, min_epochs=1, max_epochs=config.NUM_EPOCHS, precision=config.PRECISION, callbacks=[MyPrintingCallback(), EarlyStopping(monitor=\"val_loss\")])\n",
        "\n",
        "#   trainer.fit(model, dm)\n",
        "#   trainer.validate(model, dm)\n",
        "#   trainer.test(model, dm)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "logger = TensorBoardLogger(\"tb_logs\",name=\"mnist_model_v1\")\n",
        "profiler = PyTorchProfiler(\n",
        "    on_trace_ready=torch.profiler.tensorboard_trace_handler(\"tb_logs/profiler0\"),\n",
        "    schedule=torch.profiler.schedule(skip_first=10, wait=1, warmup=1, active=20)\n",
        ")\n",
        "# Initialize network\n",
        "model = NN(input_size=INPUT_SIZE, learning_rate=LEARNING_RATE, num_classes=NUM_CLASSES)\n",
        "\n",
        "# Data Module\n",
        "dm = MnistDataModule(data_dir=DATA_DIR, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n",
        "\n",
        "trainer = pl.Trainer(profiler=profiler, logger=logger, accelerator=ACCELERATOR, devices=DEVICES, min_epochs=1, max_epochs=NUM_EPOCHS, precision=PRECISION, callbacks=[MyPrintingCallback(), EarlyStopping(monitor=\"val_loss\")])\n",
        "\n",
        "trainer.fit(model, dm)\n",
        "trainer.validate(model, dm)\n",
        "trainer.test(model, dm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        },
        "id": "Rp2wSCCt1UoT",
        "outputId": "e9d73426-0484-4f52-d1db-d155b1f2b65b"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: Using 16bit Automatic Mixed Precision (AMP)\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Using 16bit Automatic Mixed Precision (AMP)\n",
            "INFO: GPU available: True (cuda), used: True\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "Support for `training_epoch_end` has been removed in v2.0.0. `NN` implements this method. You can use the `on_train_epoch_end` hook instead. To access outputs, save them in-memory as instance attributes. You can find migration examples in https://github.com/Lightning-AI/lightning/pull/16520.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-9d0d434056c6>\u001b[0m in \u001b[0;36m<cell line: 47>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccelerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mACCELERATOR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDEVICES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPRECISION\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mMyPrintingCallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"val_loss\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainerStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRUNNING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 543\u001b[0;31m         call._call_and_handle_interrupt(\n\u001b[0m\u001b[1;32m    544\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_impl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_TunerExitException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py\u001b[0m in \u001b[0;36m_fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    577\u001b[0m             \u001b[0mmodel_connected\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m         )\n\u001b[0;32m--> 579\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    934\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callback_connector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_attach_model_logging_functions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 936\u001b[0;31m         \u001b[0m_verify_loop_configurations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    937\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m         \u001b[0;31m# ----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/configuration_validator.py\u001b[0m in \u001b[0;36m_verify_loop_configurations\u001b[0;34m(trainer)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unexpected: Trainer state fn must be set before validating loop configuration.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTrainerFn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFITTING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0m__verify_train_val_loop_configuration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0m__verify_manual_optimization_support\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTrainerFn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVALIDATING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/configuration_validator.py\u001b[0m in \u001b[0;36m__verify_train_val_loop_configuration\u001b[0;34m(trainer, model)\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;31m# check legacy hooks are not present\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"training_epoch_end\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         raise NotImplementedError(\n\u001b[0m\u001b[1;32m     78\u001b[0m             \u001b[0;34mf\"Support for `training_epoch_end` has been removed in v2.0.0. `{type(model).__name__}` implements this\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;34m\" method. You can use the `on_train_epoch_end` hook instead. To access outputs, save them in-memory as\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: Support for `training_epoch_end` has been removed in v2.0.0. `NN` implements this method. You can use the `on_train_epoch_end` hook instead. To access outputs, save them in-memory as instance attributes. You can find migration examples in https://github.com/Lightning-AI/lightning/pull/16520."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b8Pni_IY-G-m"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}